{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ee53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and test minimal predictive processing model to reproduce LZ and TE changes in\n",
    "the psychedelic state and schizophrenia patients. The model consists of a Kalman filter\n",
    "trained on time series from the placebo condition of the LSD_MEG dataset, and LSD/SCH\n",
    "are modelled as increased prior/likelihood variance, respectively.\n",
    "\n",
    "Pedro Mediano and Hardik Rajpal, Apr 2021\n",
    "\"\"\"\n",
    "\n",
    "#from glob import glob\n",
    "from itertools import product\n",
    "from lz76.lz76 import LZ76\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, filtfilt, decimate\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py\n",
    "import jpype\n",
    "import matplotlib.pyplot as pl\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "import sys\n",
    "\n",
    "project_root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee7d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startJVM():\n",
    "    \"\"\"\n",
    "    Start JVM using jpype, if not already running. Assumes the JIDT jar is in\n",
    "    the current folder.\n",
    "    \"\"\"\n",
    "    jarLocation = os.path.join(project_root, \"infodynamics-dist-1.6.1\", \"infodynamics.jar\")\n",
    "    if not jpype.isJVMStarted():\n",
    "        jpype.startJVM(jpype.getDefaultJVMPath(), \"-ea\",\"-Xmx1024m\", \"-Djava.class.path=\" + jarLocation)\n",
    "\n",
    "\n",
    "def TE(src, tgt, k=1, tau=1):\n",
    "    \"\"\"\n",
    "    Computes transfer entropy using the JIDT Gaussian solver between two 1D\n",
    "    time series.\n",
    "    \"\"\"\n",
    "    startJVM()\n",
    "    te_calc = jpype.JPackage(\"infodynamics.measures.continuous.gaussian\").TransferEntropyCalculatorMultiVariateGaussian()\n",
    "    te_calc.setProperty(\"DELAY\",str(tau))\n",
    "    te_calc.setProperty(\"K\",str(k))\n",
    "    te_calc.setProperty(\"L\",str(k))\n",
    "    te_calc.initialise(1, 1)\n",
    "    te_calc.setObservations(src.tolist(), tgt.tolist())\n",
    "    te_val = te_calc.computeAverageLocalOfObservations()\n",
    "    return te_val\n",
    "\n",
    "\n",
    "def simulate(x, f):\n",
    "    \"\"\"\n",
    "    Given a KalmanFilter f and an input time series x, returns a tuple (s,o)\n",
    "    where s are the filtered states and o are the model prediction errors.\n",
    "    \"\"\"\n",
    "    s = f.filter(x)[0]\n",
    "    o = (f.observation_matrices @ s.T).T \n",
    "    o = np.array(o)\n",
    "    return (s.squeeze()[1:], o.squeeze()[1:] - x[1:])\n",
    "\n",
    "\n",
    "def LZ(x):\n",
    "    \"\"\"\n",
    "    Convenience wrapper function to compute normalised LZ of a time series\n",
    "    after detrending and binarisation.\n",
    "    \"\"\"\n",
    "    return LZ76(1*((x-np.mean(x)) > 0))*np.log(len(x))/len(x)\n",
    "\n",
    "\n",
    "def copy_filter(kf, prior_factor=1, likelihood_factor=1):\n",
    "    f = KalmanFilter(n_dim_state=kf.n_dim_state, n_dim_obs=kf.n_dim_obs,\n",
    "                     initial_state_mean=kf.initial_state_mean,\n",
    "                     initial_state_covariance = prior_factor*kf.initial_state_covariance,\n",
    "                     transition_covariance = prior_factor*kf.transition_covariance,\n",
    "                     observation_covariance = likelihood_factor*kf.observation_covariance,\n",
    "                     em_vars = ['transition_matrices','observation_matrices'])\n",
    "    return f\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=1):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=1, axis=0):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data, axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_data(eeg_source, sub_id, random_trials=True):\n",
    "    '''\n",
    "    Loads data from primary visual cortex (calcarine) for a given subject from the loaded MatLab dictionary.\n",
    "    Data is of the shape (Time x AAL x Trials), where AAL is the number of AAL regions (24 for left calcarine, 25 for right calcarine).\n",
    "    The function returns a numpy array of shape (Trials x Time) for the calcarine region.\n",
    "    If random_trials is True, it randomly selects 10 trials from the available data.\n",
    "\n",
    "    Parameters:\n",
    "    - eeg_source: dict, loaded MatLab dictionary containing all subjects' data\n",
    "    - sub_id: str, subject ID to extract data for\n",
    "    - random_trials: bool, whether to select random trials or not\n",
    "    Returns:\n",
    "    - np.ndarray: shape (Trials x Time) for the calcarine region\n",
    "    '''\n",
    "    calcarine_idx = 24  # Index for the left calcarine region in AAL atlas (24 for left, 25 for right)\n",
    "    normalise = lambda x: x/x.std()\n",
    "    \n",
    "    # Extract subject data: [samples * sources * trials]\n",
    "    subject_data = eeg_source[sub_id]\n",
    "    # Extract calcarine region data and transpose to [trials * samples]\n",
    "    calcarine_data = subject_data[:, calcarine_idx, :].T\n",
    "    \n",
    "    # Select random trials and process data\n",
    "    nb_trials = 10\n",
    "    total_trials = calcarine_data.shape[0]\n",
    "    \n",
    "    if random_trials:\n",
    "        idx = np.random.randint(total_trials, size=min(nb_trials, total_trials))\n",
    "    else:\n",
    "        idx = np.arange(min(nb_trials, total_trials))\n",
    "    \n",
    "    D = calcarine_data[idx]\n",
    "    D_proc = []\n",
    "    for i in range(len(D)): #Bandpass filtering for the data. Comment this out if data is already filtered\n",
    "        filtered_data = butter_bandpass_filter(D[i,:], lowcut=1, highcut=100, fs=600, order=3)\n",
    "        D_proc.append(normalise(filtered_data))\n",
    "    \n",
    "    return np.array(D_proc)  # Returns the processed data as a numpy array of shape (Trials x Time) for a given subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def model_run():\n",
    "    # Read the Excel file\n",
    "    file_name = 'Data_4_Import_REST.xlsx'\n",
    "    excel_sheet_name = 'Depression Rest'\n",
    "    file_path = os.path.join(\"Depression_Study\", \"depression_data\", file_name)\n",
    "    df_excel = pd.read_excel(file_path, sheet_name=excel_sheet_name)\n",
    "    df_excel['depressed'] = df_excel['MDD'].apply(lambda x: 1 if x <= 2 else 0)\n",
    "\n",
    "    # Load all subjects' source reconstructed data from a matlab file\n",
    "    data_file = os.path.join(project_root, 'eeg_source.mat')\n",
    "    eeg_source = h5py.File(data_file, 'r')\n",
    "    eeg_source_open = eeg_source['eeg_source_open']\n",
    "    eeg_source_closed = eeg_source['eeg_source_closed'] # TODO - figure out how to use this data\n",
    "    \n",
    "    # Get all subject IDs from the struct fields\n",
    "    subject_ids = [key for key in eeg_source_open.keys() if key[1:] in df_excel[df_excel['depressed'] == 0]['id'].astype(str).values]\n",
    "    subject_ids = subject_ids[:3] # TESTING - limit to 3 subjects\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # Loop over each subject\n",
    "    for z, sub_id in enumerate(tqdm(subject_ids)):\n",
    "        # print(f\"{z+1}/{len(subject_ids)}\")\n",
    "        \n",
    "        # Loads filtered and normalised data of V1 calcarine region for the subject of shape (Trials x Time)\n",
    "        D = load_data(eeg_source_open, sub_id, random_trials=True)\n",
    "        \n",
    "        ## Train baseline filter\n",
    "        kf = KalmanFilter()\n",
    "        for X in D:\n",
    "            kf.em(X, n_iter=1)\n",
    "\n",
    "        # Second round of training of baseline filter\n",
    "        kf2 = copy_filter(kf)\n",
    "        for X in D:\n",
    "            kf2.em(X, n_iter=1)\n",
    "\n",
    "        # Simulate predictions\n",
    "        sim = [simulate(x, kf2) for x in D] # Get predicted states (frontal regions signal) and prediction errors (sensory regions signal)\n",
    "        baseline_lz = np.mean([LZ(s[1]) for s in sim]) # LZ estimated for prediction errors\n",
    "        # baseline_te = np.mean([TE(s[0], s[1], k=1) for s in sim]) # TE estimated from filtered states to prediction errors - front-to-back / top-down\n",
    "        baseline_te = np.mean([TE(s[1], s[0], k=1) for s in sim]) # TE estimated from prediction errors to filtered states - back-to-front / bottom-up\n",
    "\n",
    "        factor_vec = 2.0**np.arange(-5,6,1)\n",
    "        \n",
    "        for eta_prior in factor_vec:\n",
    "            for eta_likelihood in factor_vec:\n",
    "                # print(np.round(eta,2),end=\"...\")\n",
    "\n",
    "                # Try different prior and likelihood factors to match the LZ and TE changes in the Depressed condition.\n",
    "                # Train one filter and then copy it with different prior/likelihood factors.\n",
    "            \n",
    "                ## Define model filter for depression\n",
    "                depressed_kf = copy_filter(kf, prior_factor=eta_prior, likelihood_factor=eta_likelihood)\n",
    "                # prior_factor=eta - increases prior variance - mimics overactive top-down priors\n",
    "                # likelihood_factor=eta - increases likelihood variance - mimics overactive sensory trust (or weakened frontal control)\n",
    "\n",
    "                ## Retrain the model with the same data, but with different prior/likelihood factors\n",
    "                for X in D:\n",
    "                    depressed_kf.em(X, n_iter=1)\n",
    "            \n",
    "                ## Simulate and compute LZ+TE\n",
    "                for t, x in enumerate(D):\n",
    "                    depressed_sim = simulate(x, depressed_kf)\n",
    "                    lz = LZ(depressed_sim[1])\n",
    "                    te = TE(depressed_sim[0], depressed_sim[1], k=1)\n",
    "                    df.append(pd.DataFrame({'Subject': sub_id, 'Dataset': 'EEG', 'Prior Factor': eta_prior, 'Likelihood Factor': eta_likelihood, \n",
    "                                            'Trial': t, 'LZ': lz, 'TE': te, 'Model': 'Depression'}, index=[0]))\n",
    "            \n",
    "                    # Add baseline again, for convenience\n",
    "                    df.append(pd.DataFrame({'Subject': sub_id, 'Dataset': 'EEG', 'Prior Factor': eta_prior, 'Likelihood Factor': eta_likelihood,\n",
    "                                            'Trial': t, 'LZ': baseline_lz, 'TE': baseline_te, 'Model': 'Baseline'}, index=[0]))\n",
    "            # print(\"\\n\")\n",
    "    \n",
    "    return pd.concat(df, ignore_index=True)\n",
    "\n",
    "\n",
    "df_raw_double = model_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to pickle files\n",
    "with open('df_raw_double.pkl', 'wb') as f:\n",
    "    pickle.dump(df_raw_double, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86009112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run():\n",
    "    # Read the Excel file\n",
    "    file_name = 'Data_4_Import_REST.xlsx'\n",
    "    excel_sheet_name = 'Depression Rest'\n",
    "    file_path = os.path.join(\"Depression_Study\", \"depression_data\", file_name)\n",
    "    df_excel = pd.read_excel(file_path, sheet_name=excel_sheet_name)\n",
    "    df_excel['depressed'] = df_excel['MDD'].apply(lambda x: 1 if x <= 2 else 0)\n",
    "\n",
    "    # Load all subjects' source reconstructed data from a matlab file\n",
    "    data_file = os.path.join(project_root, 'eeg_source.mat')\n",
    "    eeg_source = h5py.File(data_file, 'r')\n",
    "    eeg_source_open = eeg_source['eeg_source_open']\n",
    "    eeg_source_closed = eeg_source['eeg_source_closed'] # TODO - figure out how to use this data\n",
    "    \n",
    "    # Get all subject IDs from the struct fields\n",
    "    subject_ids = [key for key in eeg_source_open.keys() if key[1:] in df_excel[df_excel['depressed'] == 0]['id'].astype(str).values]\n",
    "    subject_ids = subject_ids[:3] # TESTING - limit to 3 subjects\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # Loop over each subject\n",
    "    for z, sub_id in enumerate(tqdm(subject_ids)):\n",
    "        # print(f\"{z+1}/{len(subject_ids)}\")\n",
    "        \n",
    "        # Loads filtered and normalised data of V1 calcarine region for the subject of shape (Trials x Time)\n",
    "        D = load_data(eeg_source_open, sub_id, random_trials=True)\n",
    "        \n",
    "        ## Train baseline filter\n",
    "        kf = KalmanFilter()\n",
    "        for X in D:\n",
    "            kf.em(X, n_iter=1)\n",
    "\n",
    "        # Second round of training of baseline filter\n",
    "        kf2 = copy_filter(kf)\n",
    "        for X in D:\n",
    "            kf2.em(X, n_iter=1)\n",
    "\n",
    "        # Simulate predictions\n",
    "        sim = [simulate(x, kf2) for x in D] # Get predicted states (frontal regions signal) and prediction errors (sensory regions signal)\n",
    "        baseline_lz = np.mean([LZ(s[1]) for s in sim]) # LZ estimated for prediction errors\n",
    "        # baseline_te = np.mean([TE(s[0], s[1], k=1) for s in sim]) # TE estimated from filtered states to prediction errors - front-to-back / top-down\n",
    "        baseline_te = np.mean([TE(s[1], s[0], k=1) for s in sim]) # TE estimated from prediction errors to filtered states - back-to-front / bottom-up\n",
    "\n",
    "        factor_vec = 2.0**np.arange(-5,6,1)\n",
    "        \n",
    "        for eta in factor_vec:\n",
    "            # print(np.round(eta,2),end=\"...\")\n",
    "\n",
    "            # Try different prior and likelihood factors to match the LZ and TE changes in the Depressed condition.\n",
    "            # Train one filter and then copy it with different prior/likelihood factors.\n",
    "        \n",
    "            ## Define model filter for depression\n",
    "            prior_kf = copy_filter(kf, prior_factor=eta)\n",
    "            likelihood_kf = copy_filter(kf, likelihood_factor=eta)\n",
    "            # prior_factor=eta - increases prior variance - mimics overactive top-down priors\n",
    "            # likelihood_factor=eta - increases likelihood variance - mimics overactive sensory trust (or weakened frontal control)\n",
    "\n",
    "            ## Retrain the model with the same data, but with different prior/likelihood factors\n",
    "            for X in D:\n",
    "                prior_kf.em(X, n_iter=1)\n",
    "                likelihood_kf.em(X, n_iter=1)\n",
    "        \n",
    "            ## Simulate and compute LZ+TE\n",
    "            for t, x in enumerate(D):\n",
    "                prior_sim = simulate(x, prior_kf)\n",
    "                lz = LZ(prior_sim[1])\n",
    "                te = TE(prior_sim[0], prior_sim[1], k=1)\n",
    "                df.append(pd.DataFrame({'Subject': sub_id, 'Dataset': 'EEG', 'Prior Factor': eta, 'Likelihood Factor': eta,\n",
    "                                        'Trial': t, 'LZ': lz, 'TE': te, 'Model': 'Prior'}, index=[0]))\n",
    "                \n",
    "                likelihood_sim = simulate(x, likelihood_kf)\n",
    "                lz = LZ(likelihood_sim[1])\n",
    "                te = TE(likelihood_sim[0], likelihood_sim[1], k=1)\n",
    "                df.append(pd.DataFrame({'Subject': sub_id, 'Dataset': 'EEG', 'Prior Factor': eta, 'Likelihood Factor': eta,\n",
    "                                        'Trial': t, 'LZ': lz, 'TE': te, 'Model': 'Likelihood'}, index=[0]))\n",
    "        \n",
    "                # Add baseline again, for convenience\n",
    "                df.append(pd.DataFrame({'Subject': sub_id, 'Dataset': 'EEG', 'Prior Factor': eta, 'Likelihood Factor': eta,\n",
    "                                        'Trial': t, 'LZ': baseline_lz, 'TE': baseline_te, 'Model': 'Baseline'}, index=[0]))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "    return pd.concat(df, ignore_index=True)\n",
    "\n",
    "\n",
    "df_raw_single = model_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c34d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to pickle files\n",
    "with open('df_raw_single.pkl', 'wb') as f:\n",
    "    pickle.dump(df_raw_single, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from pickle file\n",
    "df_raw_double = None\n",
    "with open('df_raw_double.pkl', 'rb') as f:\n",
    "    df_raw_double = pickle.load(f)\n",
    "\n",
    "df_raw_single = None\n",
    "with open('df_raw_single.pkl', 'rb') as f:\n",
    "    df_raw_single = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0c897",
   "metadata": {},
   "source": [
    "## Data Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw_double.copy()\n",
    "df = df_raw_single.copy()\n",
    "\n",
    "df = df.melt(id_vars=['Subject', 'Dataset', 'Prior Factor', 'Likelihood Factor', 'Model', 'Trial'],#, 'Run'],\n",
    "                value_name='Value', var_name='Measure')\n",
    "\n",
    "df.to_csv(\"model_results_latest_proc10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394be3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot varying Prior Factor (with fixed Likelihood Factor = 1.0)\n",
    "df_prior = df[df['Likelihood Factor'] == 1.0]\n",
    "\n",
    "pl.figure(1)\n",
    "g = sn.relplot(col='Measure', y='Value', hue='Model', x='Prior Factor', ci=60,\n",
    "                kind='line', facet_kws={'sharey': False}, data=df_prior)\n",
    "g.axes[0,0].set_xscale('log', base=2)\n",
    "g.figure.suptitle('Effect of Prior Factor (Likelihood Factor = 1.0)', y=1.02)\n",
    "pl.tight_layout()\n",
    "\n",
    "# Plot varying Likelihood Factor (with fixed Prior Factor = 1.0)\n",
    "df_likelihood = df[df['Prior Factor'] == 1.0]\n",
    "\n",
    "pl.figure(2)\n",
    "g = sn.relplot(col='Measure', y='Value', hue='Model', x='Likelihood Factor', ci=60,\n",
    "                kind='line', facet_kws={'sharey': False}, data=df_likelihood)\n",
    "g.axes[0,0].set_xscale('log', base=2)\n",
    "g.figure.suptitle('Effect of Likelihood Factor (Prior Factor = 1.0)', y=1.02)\n",
    "pl.tight_layout()\n",
    "\n",
    "df_depression = df.loc[df['Model']=='Depression']\n",
    "df_depression['Model'] = 'Depression'\n",
    "\n",
    "df_base = df.loc[df['Model']=='Baseline']\n",
    "df_all = pd.concat([df_depression, df_base], ignore_index=True)\n",
    "df_all.to_csv('model_results_final_proc10.csv')\n",
    "\n",
    "# Combined plot with both models - Prior Factor\n",
    "df_all_prior = df_all[df_all['Likelihood Factor'] == 1.0]\n",
    "\n",
    "pl.figure(3)\n",
    "g = sn.relplot(col='Measure', y='Value', hue='Model', x='Prior Factor', ci=60,\n",
    "                kind='line', facet_kws={'sharey': False}, data=df_all_prior)\n",
    "g.axes[0,0].set_xscale('log', base=2)\n",
    "g.figure.suptitle('Comparison: Prior Factor Effect (Likelihood Factor = 1.0)', y=1.02)\n",
    "pl.tight_layout()\n",
    "\n",
    "# Combined plot with both models - Likelihood Factor\n",
    "df_all_likelihood = df_all[df_all['Prior Factor'] == 1.0]\n",
    "\n",
    "pl.figure(4)\n",
    "g = sn.relplot(col='Measure', y='Value', hue='Model', x='Likelihood Factor', ci=60,\n",
    "                kind='line', facet_kws={'sharey': False}, data=df_all_likelihood)\n",
    "g.axes[0,0].set_xscale('log', base=2)\n",
    "g.figure.suptitle('Comparison: Likelihood Factor Effect (Prior Factor = 1.0)', y=1.02)\n",
    "pl.tight_layout()\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a49cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "# Filter only the depression model (exclude baseline if needed)\n",
    "df_dep = df[df['Model'] == 'Depression']\n",
    "\n",
    "# Compute mean across trials and subjects\n",
    "agg = df_dep.groupby(['PriorFactor', 'LikelihoodFactor']).agg({'TE': 'mean', 'LZ': 'mean'}).reset_index()\n",
    "\n",
    "# Pivot for heatmaps\n",
    "te_pivot = agg.pivot(index='LikelihoodFactor', columns='PriorFactor', values='TE')\n",
    "lz_pivot = agg.pivot(index='LikelihoodFactor', columns='PriorFactor', values='LZ')\n",
    "\n",
    "pl.figure(figsize=(8,6))\n",
    "sn.heatmap(te_pivot, cmap='viridis', annot=True, fmt=\".3f\", cbar_kws={'label': 'Transfer Entropy'})\n",
    "pl.title(\"Transfer Entropy (Prediction Errors → Hidden States)\")\n",
    "pl.xlabel(\"Prior Factor\")\n",
    "pl.ylabel(\"Likelihood Factor\")\n",
    "pl.tight_layout()\n",
    "pl.show()\n",
    "\n",
    "pl.figure(figsize=(8,6))\n",
    "sn.heatmap(lz_pivot, cmap='magma', annot=True, fmt=\".3f\", cbar_kws={'label': 'Lempel-Ziv Complexity'})\n",
    "pl.title(\"LZ Complexity of Prediction Errors\")\n",
    "pl.xlabel(\"Prior Factor\")\n",
    "pl.ylabel(\"Likelihood Factor\")\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
